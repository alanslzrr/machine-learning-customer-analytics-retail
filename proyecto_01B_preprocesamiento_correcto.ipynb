{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "353aac70",
   "metadata": {},
   "source": [
    "\n",
    "# proyecto_01B_preprocesamiento_correcto\n",
    "\n",
    "Notebook para preparar los datos y entrenar pipelines sin data leakage. Incluye limpieza determinística, split antes de transformaciones estadísticas y guardado del pipeline final.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bc366c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from scipy.stats import ks_2samp\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    classification_report,\n",
    "    average_precision_score,\n",
    "    precision_recall_curve,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    r2_score,\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    ")\n",
    "from sklearn.base import clone\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce9e302",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Carga y limpieza determinística (sin fits)\n",
    "- Eliminación de outliers evidentes (edad >120, ingresos = 666666)\n",
    "- Cálculo de `ratio_compras_online` y eliminación de clientes sin compras\n",
    "- Ingeniería básica de características determinísticas\n",
    "- Exportación de un CSV limpio sin escalado\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9cdd97f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset limpio guardado en data/interim/supermercado_limpio.csv con forma (1982, 48)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "RAW_FILE = Path(\"data/interim/supermercado_features.csv\")\n",
    "CLEAN_FILE = Path(\"data/interim/supermercado_limpio.csv\")\n",
    "\n",
    "# Carga\n",
    "df = pd.read_csv(RAW_FILE)\n",
    "df[\"fecha_cliente\"] = pd.to_datetime(df[\"fecha_cliente\"])\n",
    "\n",
    "# Reglas determinísticas\n",
    "mask_edad = df[\"edad\"] <= 120\n",
    "mask_ingreso = df[\"ingresos\"] != 666666\n",
    "mask_basico = mask_edad & mask_ingreso\n",
    "\n",
    "df = df[mask_basico].copy()\n",
    "\n",
    "# Ratio de compras y clientes activos (evita inf y NaN)\n",
    "df[\"ratio_compras_online\"] = np.where(\n",
    "    df[\"compras_totales\"] > 0,\n",
    "    df[\"compras_online\"] / df[\"compras_totales\"],\n",
    "    np.nan,\n",
    ")\n",
    "df = df.dropna(subset=[\"ratio_compras_online\"])\n",
    "\n",
    "# Ingeniería básica\n",
    "educ_map = {\"Basica\": 1, \"Secundaria\": 2, \"Universitaria\": 3, \"Master\": 4, \"Doctorado\": 5}\n",
    "df[\"tiene_pareja\"] = df[\"estado_civil\"].isin([\"Casado\", \"Union_Libre\"]).astype(int)\n",
    "df[\"educacion\"] = df[\"educacion\"].map(educ_map)\n",
    "df[\"educacion_x_estado\"] = df[\"educacion\"] * df[\"tiene_pareja\"]\n",
    "df[\"hijos_casa\"] = df[\"hijos_casa\"] + df[\"adolescentes_casa\"]\n",
    "df[\"anio_alta\"] = df[\"fecha_cliente\"].dt.year\n",
    "\n",
    "# Eliminar columnas redundantes\n",
    "cols_drop = [\"total_dependientes\", \"compras_online\", \"adolescentes_casa\", \"anio_nacimiento\", \"fecha_cliente\"]\n",
    "df = df.drop(columns=[c for c in cols_drop if c in df.columns])\n",
    "\n",
    "# Guardar limpio sin escalado\n",
    "CLEAN_FILE.parent.mkdir(parents=True, exist_ok=True)\n",
    "df.to_csv(CLEAN_FILE, index=False)\n",
    "print(f\"Dataset limpio guardado en {CLEAN_FILE} con forma {df.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ecc839",
   "metadata": {},
   "source": [
    "> Nota de referencia de datos: `data/interim/supermercado_limpio.csv` es el insumo oficial para entrenamiento e inferencia (sin log1p ni escalado). El archivo `data/processed/supermercado_preprocesado.csv` del notebook 01 se mantiene solo para exploración/EDA con log1p aplicado; no debe usarse para servir modelos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0b84c2",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Split train/test antes de transformaciones\n",
    "- One-hot encoding determinístico\n",
    "- Estratificación por `respuesta`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "90f24b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (1508, 47), Test: (474, 47)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Separación de variables\n",
    "X = df.drop(\"respuesta\", axis=1)\n",
    "y = df[\"respuesta\"]\n",
    "\n",
    "categorical_cols = X.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "binary_cols = [\n",
    "    c for c in X.columns\n",
    "    if c not in categorical_cols and set(X[c].dropna().unique()).issubset({0, 1})\n",
    "]\n",
    "numeric_cols = [c for c in X.columns if c not in categorical_cols]\n",
    "continuous_cols = [c for c in numeric_cols if c not in binary_cols]\n",
    "\n",
    "# Split temporal: train antes de 2014, test 2014 en adelante\n",
    "train_mask = df[\"anio_alta\"] < 2014\n",
    "X_train = X.loc[train_mask].copy()\n",
    "X_test = X.loc[~train_mask].copy()\n",
    "y_train = y.loc[train_mask].copy()\n",
    "y_test = y.loc[~train_mask].copy()\n",
    "\n",
    "print(f\"Train: {X_train.shape}, Test: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5642584f",
   "metadata": {},
   "source": [
    "## 2.1 Chequeo de deriva temporal (train <2014 vs test ≥2014)\n",
    "\n",
    "Comprobamos si cambian las distribuciones de variables clave entre train y test. Si el KS test muestra p-val < 0.05, hay indicios de deriva que pueden requerir recalibración o ajuste de umbral.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d0a7646e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variable</th>\n",
       "      <th>media_train</th>\n",
       "      <th>media_test</th>\n",
       "      <th>std_train</th>\n",
       "      <th>std_test</th>\n",
       "      <th>ks_stat</th>\n",
       "      <th>p_valor</th>\n",
       "      <th>deriva</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ingresos</td>\n",
       "      <td>51289.062997</td>\n",
       "      <td>52254.506329</td>\n",
       "      <td>20406.423213</td>\n",
       "      <td>21433.321275</td>\n",
       "      <td>0.058375</td>\n",
       "      <td>0.163320</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>recencia</td>\n",
       "      <td>49.079576</td>\n",
       "      <td>48.046414</td>\n",
       "      <td>28.230594</td>\n",
       "      <td>29.627175</td>\n",
       "      <td>0.060765</td>\n",
       "      <td>0.132750</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ratio_compras_online</td>\n",
       "      <td>0.413944</td>\n",
       "      <td>0.389195</td>\n",
       "      <td>0.144375</td>\n",
       "      <td>0.144908</td>\n",
       "      <td>0.112740</td>\n",
       "      <td>0.000187</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tasa_compra_online</td>\n",
       "      <td>0.413983</td>\n",
       "      <td>0.389213</td>\n",
       "      <td>0.144370</td>\n",
       "      <td>0.144897</td>\n",
       "      <td>0.112740</td>\n",
       "      <td>0.000187</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               variable   media_train    media_test     std_train  \\\n",
       "0              ingresos  51289.062997  52254.506329  20406.423213   \n",
       "1              recencia     49.079576     48.046414     28.230594   \n",
       "2  ratio_compras_online      0.413944      0.389195      0.144375   \n",
       "3    tasa_compra_online      0.413983      0.389213      0.144370   \n",
       "\n",
       "       std_test   ks_stat   p_valor  deriva  \n",
       "0  21433.321275  0.058375  0.163320   False  \n",
       "1     29.627175  0.060765  0.132750   False  \n",
       "2      0.144908  0.112740  0.000187    True  \n",
       "3      0.144897  0.112740  0.000187    True  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars_drift = [\n",
    "    \"ingresos\",\n",
    "    \"recencia\",\n",
    "    \"ratio_compras_online\",\n",
    "    \"tasa_compra_online\",\n",
    "]\n",
    "\n",
    "vars_drift = [v for v in vars_drift if v in X_train.columns]\n",
    "\n",
    "rows = []\n",
    "for col in vars_drift:\n",
    "    train_vals = X_train[col].dropna()\n",
    "    test_vals = X_test[col].dropna()\n",
    "    ks_stat, p_val = ks_2samp(train_vals, test_vals)\n",
    "    rows.append(\n",
    "        {\n",
    "            \"variable\": col,\n",
    "            \"media_train\": train_vals.mean(),\n",
    "            \"media_test\": test_vals.mean(),\n",
    "            \"std_train\": train_vals.std(),\n",
    "            \"std_test\": test_vals.std(),\n",
    "            \"ks_stat\": ks_stat,\n",
    "            \"p_valor\": p_val,\n",
    "            \"deriva\": p_val < 0.05,\n",
    "        }\n",
    "    )\n",
    "\n",
    "chequeo_deriva = pd.DataFrame(rows)\n",
    "chequeo_deriva\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fe4875",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Pipeline de preprocesamiento\n",
    "- Transformación log1p en continuas (opcional para modelos lineales)\n",
    "- Estandarización solo en train\n",
    "- PCA opcional (comentado por defecto)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959d251c",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Pipeline de preprocesamiento\n",
    "- Transformación log1p en continuas (opcional para modelos lineales)\n",
    "- Estandarización solo en train\n",
    "- PCA opcional (comentado por defecto)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b8a8e79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "categorical_encoder = OneHotEncoder(\n",
    "    handle_unknown=\"ignore\",\n",
    "    drop=\"first\",\n",
    "    sparse_output=False,  # Cambiado de 'sparse' a 'sparse_output'\n",
    ")\n",
    "\n",
    "continuous_pipeline = Pipeline([\n",
    "    (\"log1p\", FunctionTransformer(np.log1p, validate=False)),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    # (\"pca\", PCA(n_components=0.80))  # Activar si se requiere\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", categorical_encoder, categorical_cols),\n",
    "        (\"cont\", continuous_pipeline, continuous_cols),\n",
    "        (\"bin\", \"passthrough\", binary_cols),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    "    verbose_feature_names_out=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd065cbd",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Entrenamiento y evaluación de clasificación sin leakage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4fb0e06c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Umbral fijo para inferencia: 0.350 (prioridad al recall)\n",
      "Validación (20% train) -> Precision: 0.420 | Recall: 0.894 | F1: 0.571\n",
      "AUC Train: 0.9240 | AUC Test: 0.9535 | Gap: -0.0295\n",
      "AUCPR Test : 0.6061\n",
      "CV AUC (5-fold): 0.8949 ± 0.0182\n",
      "Classification report (test, umbral fijo de inferencia):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.92      0.95       431\n",
      "           1       0.50      0.81      0.62        43\n",
      "\n",
      "    accuracy                           0.91       474\n",
      "   macro avg       0.74      0.87      0.78       474\n",
      "weighted avg       0.94      0.91      0.92       474\n",
      "\n",
      "Accuracy test: 0.9093\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Clasificación: Logistic Regression con pesos {0:1, 1:7} y umbral fijo priorizando recall\n",
    "clf = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"model\", LogisticRegression(class_weight={0: 1, 1: 7}, max_iter=1000, random_state=42)),\n",
    "])\n",
    "\n",
    "# Split de validación (20% del train) para monitorear el umbral sin usar test\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "clf_val = clone(clf)\n",
    "clf_val.fit(X_tr, y_tr)\n",
    "\n",
    "y_proba_val = clf_val.predict_proba(X_val)[:, 1]\n",
    "val_threshold = 0.35  # Umbral operativo elegido tras tuning enfocado en recall\n",
    "\n",
    "y_pred_val = (y_proba_val >= val_threshold).astype(int)\n",
    "val_precision = precision_score(y_val, y_pred_val)\n",
    "val_recall = recall_score(y_val, y_pred_val)\n",
    "val_f1 = f1_score(y_val, y_pred_val)\n",
    "\n",
    "# Entrenamiento final en todo el train\n",
    "clf = clone(clf)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_proba_train = clf.predict_proba(X_train)[:, 1]\n",
    "y_proba_test = clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "a = roc_auc_score(y_train, y_proba_train)\n",
    "b = roc_auc_score(y_test, y_proba_test)\n",
    "auprc = average_precision_score(y_test, y_proba_test)\n",
    "\n",
    "cv_scores_auc = cross_val_score(clf, X_train, y_train, cv=5, scoring=\"roc_auc\")\n",
    "\n",
    "print(f\"Umbral fijo para inferencia: {val_threshold:.3f} (prioridad al recall)\")\n",
    "print(f\"Validación (20% train) -> Precision: {val_precision:.3f} | Recall: {val_recall:.3f} | F1: {val_f1:.3f}\")\n",
    "print(f\"AUC Train: {a:.4f} | AUC Test: {b:.4f} | Gap: {a-b:.4f}\")\n",
    "print(f\"AUCPR Test : {auprc:.4f}\")\n",
    "print(f\"CV AUC (5-fold): {cv_scores_auc.mean():.4f} ± {cv_scores_auc.std():.4f}\")\n",
    "\n",
    "# Reporte con umbral fijado\n",
    "\n",
    "y_pred_test_opt = (y_proba_test >= val_threshold).astype(int)\n",
    "acc_test = accuracy_score(y_test, y_pred_test_opt)\n",
    "\n",
    "print(\"Classification report (test, umbral fijo de inferencia):\")\n",
    "print(classification_report(y_test, y_pred_test_opt))\n",
    "print(f\"Accuracy test: {acc_test:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52893bfb",
   "metadata": {},
   "source": [
    "\n",
    "### Interpretación clasificación (sin leakage, split temporal)\n",
    "- Modelo final: LogisticRegression con class_weight={0:1, 1:7} para penalizar más los falsos negativos.\n",
    "- Umbral operativo fijo 0.35 priorizando recall (seleccionado por tuning externo); se monitorea en una validación 20% del train para evitar fuga.\n",
    "- AUC train/test y AUCPR se mantienen altos; el gap es controlado y se valida estabilidad con CV AUC 5-fold.\n",
    "- Este pipeline y umbral son la referencia para producción; cualquier ajuste futuro debe recalcular métricas en el corte temporal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164d8e10",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Entrenamiento y evaluación de regresión sin leakage\n",
    "Usa el mismo preprocesador; ajusta el objetivo `gasto_total` en log1p.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72878cf4",
   "metadata": {},
   "source": [
    "### Interpretación regresión (sin fuga, split temporal)\n",
    "- R² train/test: 0.939 / 0.867 con MAE ≈ 127 y RMSE ≈ 206: métricas más realistas tras eliminar features derivadas del gasto.\n",
    "- El corte temporal (<2014 / 2014+) evita fuga por tiempo; aún hay error alto, sugiere modelos más robustos o features adicionales no fugas (p.ej., ingresos, recencia, ratio_compras_online).\n",
    "- Se mantiene log1p+escalado en train vía pipeline para consistencia.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4b6ed387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 train: 0.9102\n",
      "R2 test : 0.8926\n",
      "MAE test: 95.4121\n",
      "RMSE test: 185.3552\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Preparar datos de regresión sin fuga y con corte temporal\n",
    "leak_cols = [\n",
    "    \"gasto_vinos\", \"gasto_frutas\", \"gasto_carnes\", \"gasto_pescado\", \"gasto_dulces\", \"gasto_oro\",\n",
    "    \"gasto_promedio\", \"prop_gasto_vinos\", \"prop_gasto_frutas\", \"prop_gasto_carnes\", \"prop_gasto_pescado\",\n",
    "    \"prop_gasto_dulces\", \"prop_gasto_oro\", \"ticket_promedio\", \"gasto_x_recencia\"\n",
    "]\n",
    "y_reg = np.log1p(df[\"gasto_total\"])\n",
    "X_reg_raw = df.drop([\"respuesta\", \"gasto_total\"], axis=1)\n",
    "X_reg_raw = X_reg_raw.drop(columns=[c for c in leak_cols if c in X_reg_raw.columns])\n",
    "\n",
    "categorical_cols_reg = X_reg_raw.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "binary_cols_reg = [\n",
    "    c for c in X_reg_raw.columns\n",
    "    if c not in categorical_cols_reg and set(X_reg_raw[c].dropna().unique()).issubset({0, 1})\n",
    "]\n",
    "numeric_cols_reg = [c for c in X_reg_raw.columns if c not in categorical_cols_reg]\n",
    "continuous_cols_reg = [c for c in numeric_cols_reg if c not in binary_cols_reg]\n",
    "\n",
    "train_mask_reg = df[\"anio_alta\"] < 2014\n",
    "Xr_train = X_reg_raw.loc[train_mask_reg].copy()\n",
    "Xr_test = X_reg_raw.loc[~train_mask_reg].copy()\n",
    "yr_train = y_reg.loc[train_mask_reg].copy()\n",
    "yr_test = y_reg.loc[~train_mask_reg].copy()\n",
    "\n",
    "# Cambia el parámetro 'sparse' a 'sparse_output' para compatibilidad con versiones recientes de scikit-learn.\n",
    "categorical_encoder_reg = OneHotEncoder(\n",
    "    handle_unknown=\"ignore\",\n",
    "    drop=\"first\",\n",
    "    sparse_output=False,\n",
    ")\n",
    "\n",
    "continuous_pipeline_reg = Pipeline([\n",
    "    (\"log1p\", FunctionTransformer(np.log1p, validate=False)),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "])\n",
    "\n",
    "preprocessor_reg = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", categorical_encoder_reg, categorical_cols_reg),\n",
    "        (\"cont\", continuous_pipeline_reg, continuous_cols_reg),\n",
    "        (\"bin\", \"passthrough\", binary_cols_reg),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    "    verbose_feature_names_out=False\n",
    ")\n",
    "\n",
    "reg = Pipeline([\n",
    "    (\"preprocessor\", preprocessor_reg),\n",
    "    (\"model\", GradientBoostingRegressor(random_state=42))\n",
    "])\n",
    "reg.fit(Xr_train, yr_train)\n",
    "\n",
    "pred_train_log = reg.predict(Xr_train)\n",
    "pred_test_log = reg.predict(Xr_test)\n",
    "\n",
    "# Convertir predicciones y objetivos a escala original para métricas interpretables\n",
    "pred_train = np.expm1(pred_train_log)\n",
    "pred_test = np.expm1(pred_test_log)\n",
    "yr_train_orig = np.expm1(yr_train)\n",
    "yr_test_orig = np.expm1(yr_test)\n",
    "\n",
    "print(f\"R2 train: {r2_score(yr_train_orig, pred_train):.4f}\")\n",
    "print(f\"R2 test : {r2_score(yr_test_orig, pred_test):.4f}\")\n",
    "print(f\"MAE test: {mean_absolute_error(yr_test_orig, pred_test):.4f}\")\n",
    "print(f\"RMSE test: {np.sqrt(mean_squared_error(yr_test_orig, pred_test)):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a45908",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Guardado de pipelines y metadatos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c7228bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipelines y metadatos guardados en models/\n"
     ]
    }
   ],
   "source": [
    "\n",
    "models_dir = Path(\"models\")\n",
    "models_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "joblib.dump(clf, models_dir / \"pipeline_clasificacion_sin_leakage.pkl\")\n",
    "joblib.dump(reg, models_dir / \"pipeline_regresion_sin_leakage.pkl\")\n",
    "\n",
    "# Usar los preprocesadores ya entrenados dentro de los pipelines\n",
    "preproc_clf = clf.named_steps[\"preprocessor\"]\n",
    "preproc_reg = reg.named_steps[\"preprocessor\"]\n",
    "\n",
    "# Extraer nombres de features sin depender de get_feature_names_out en pipelines con FunctionTransformer\n",
    "cat_names = preproc_clf.named_transformers_[\"cat\"].get_feature_names_out(categorical_cols)\n",
    "cont_names = continuous_cols\n",
    "bin_names = binary_cols\n",
    "feature_names_input = list(cat_names) + cont_names + bin_names\n",
    "\n",
    "cat_names_reg = preproc_reg.named_transformers_[\"cat\"].get_feature_names_out(categorical_cols_reg)\n",
    "cont_names_reg = continuous_cols_reg\n",
    "bin_names_reg = binary_cols_reg\n",
    "reg_feature_names_input = list(cat_names_reg) + cont_names_reg + bin_names_reg\n",
    "\n",
    "# Métricas de clasificación (umbral fijo)\n",
    "clf_y_pred_opt = (y_proba_test >= val_threshold).astype(int)\n",
    "clf_precision = precision_score(y_test, clf_y_pred_opt)\n",
    "clf_recall = recall_score(y_test, clf_y_pred_opt)\n",
    "clf_f1 = f1_score(y_test, clf_y_pred_opt)\n",
    "clf_accuracy = accuracy_score(y_test, clf_y_pred_opt)\n",
    "auc_train = float(a)\n",
    "auc_test = float(b)\n",
    "aucpr_test = float(auprc)\n",
    "cv_auc_mean = float(cv_scores_auc.mean())\n",
    "cv_auc_std = float(cv_scores_auc.std())\n",
    "\n",
    "# Métricas de regresión (escala original)\n",
    "r2_train = r2_score(yr_train_orig, pred_train)\n",
    "r2_test = r2_score(yr_test_orig, pred_test)\n",
    "mae_reg = mean_absolute_error(yr_test_orig, pred_test)\n",
    "rmse_reg = np.sqrt(mean_squared_error(yr_test_orig, pred_test))\n",
    "\n",
    "metadata = {\n",
    "    \"feature_names_input\": feature_names_input,\n",
    "    \"raw_feature_names\": X_train.columns.tolist(),\n",
    "    \"continuous_cols\": continuous_cols,\n",
    "    \"binary_cols\": binary_cols,\n",
    "    \"categorical_original\": categorical_cols,\n",
    "    \"train_shape\": X_train.shape,\n",
    "    \"test_shape\": X_test.shape,\n",
    "    \"reg_feature_names_input\": reg_feature_names_input,\n",
    "    \"reg_raw_feature_names\": Xr_train.columns.tolist(),\n",
    "    \"reg_continuous_cols\": continuous_cols_reg,\n",
    "    \"reg_binary_cols\": binary_cols_reg,\n",
    "    \"categorical_cols_reg\": categorical_cols_reg,\n",
    "    \"temporal_split\": {\"train_years\": \"<2014\", \"test_years\": \"2014+\"},\n",
    "    \"clf_threshold\": float(val_threshold),\n",
    "    \"metrics\": {\n",
    "        \"auc_train\": auc_train,\n",
    "        \"auc_test\": auc_test,\n",
    "        \"aucpr_test\": aucpr_test\n",
    "    },\n",
    "    \"clf_config\": {\n",
    "        \"model\": \"LogisticRegression\",\n",
    "        \"class_weight\": {0: 1, 1: 7},\n",
    "        \"threshold\": float(val_threshold),\n",
    "        \"optimized_for\": \"recall_min_60_percent\",\n",
    "    },\n",
    "    \"clf_metrics\": {\n",
    "        \"threshold_opt\": float(val_threshold),\n",
    "        \"auc_train\": auc_train,\n",
    "        \"auc_test\": auc_test,\n",
    "        \"aucpr_test\": aucpr_test,\n",
    "        \"precision_test\": float(clf_precision),\n",
    "        \"recall_test\": float(clf_recall),\n",
    "        \"f1_test\": float(clf_f1),\n",
    "        \"accuracy_test\": float(clf_accuracy),\n",
    "        \"cv_auc_mean\": cv_auc_mean,\n",
    "        \"cv_auc_std\": cv_auc_std,\n",
    "    },\n",
    "    \"reg_metrics\": {\n",
    "        \"r2_train\": float(r2_train),\n",
    "        \"r2_test\": float(r2_test),\n",
    "        \"mae_test\": float(mae_reg),\n",
    "        \"rmse_test\": float(rmse_reg)\n",
    "    }\n",
    "}\n",
    "joblib.dump(metadata, models_dir / \"pipeline_metadata.pkl\")\n",
    "print(\"Pipelines y metadatos guardados en models/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cf63f3",
   "metadata": {},
   "source": [
    "## REGRESIÓN (Predicción de Gasto)\n",
    "\n",
    "### Modelo Final\n",
    "**Gradient Boosting Regressor**\n",
    "\n",
    "### Métricas de Desempeño (Escala Original en Euros)\n",
    "| Métrica | Train | Test | Interpretación |\n",
    "|---------|-------|------|----------------|\n",
    "| **R²** | 0.9390 | **0.8670** | Explica 86.7% de varianza en test |\n",
    "| **MAE** | - | **127.44€** | Error promedio de predicción |\n",
    "| **RMSE** | - | **206.47€** | Error cuadrático medio |\n",
    "\n",
    "### Análisis de Overfitting\n",
    "- **Gap R²**: 0.9390 - 0.8670 = **0.072** (7.2%)\n",
    "- **Estado**: Ligero overfitting, pero dentro de rango aceptable\n",
    "- El modelo generaliza bien a datos no vistos\n",
    "\n",
    "### Interpretación de Negocio\n",
    "- **MAE 127€**: En promedio, las predicciones se desvían ±127€ del gasto real\n",
    "- **RMSE 206€**: Errores grandes (outliers) son penalizados más fuertemente\n",
    "- Para un cliente con gasto promedio de ~600€, esto representa un **error relativo del 21%**\n",
    "\n",
    "### Variables Clave Eliminadas (Sin Leakage)\n",
    "Se excluyeron correctamente variables derivadas del gasto:\n",
    "- `gasto_vinos`, `gasto_frutas`, `gasto_carnes`, etc.\n",
    "- `gasto_promedio`, `prop_gasto_*`\n",
    "- `ticket_promedio`, `gasto_x_recencia`\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
