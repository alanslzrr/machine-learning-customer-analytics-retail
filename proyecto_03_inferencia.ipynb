{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proyecto Final — Inferencia sobre Datos Nuevos\n",
    "\n",
    "En este notebook aplicamos los modelos entrenados sobre datos nuevos para evaluar su capacidad de generalización. El flujo garantiza coherencia con el entrenamiento mediante la reutilización de pipelines y metadatos, permitiendo inferencia reproducible sobre cualquier dataset con la estructura esperada.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuración y dependencias\n",
    "\n",
    "Definimos las rutas a los artefactos entrenados y al dataset de inferencia. Para aplicar el modelo sobre datos nuevos, modificamos `DATA_FILE` apuntando al archivo correspondiente manteniendo la misma estructura de columnas del entrenamiento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal, Tuple\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuración de la tarea\n",
    "TAREA: Literal[\"clasificacion\", \"regresion\"] = \"clasificacion\"\n",
    "\n",
    "# Rutas - Modificar DATA_FILE para aplicar sobre datos nuevos\n",
    "DATA_FILE = Path(\"data/proy_supermercado_test.csv\")\n",
    "\n",
    "MODEL_FILE = Path(\"models/pipeline_clasificacion_sin_leakage.pkl\") if TAREA == \"clasificacion\" else Path(\"models/pipeline_regresion_sin_leakage.pkl\")\n",
    "\n",
    "METADATA_FILE = Path(\"models/pipeline_metadata.pkl\")\n",
    "TARGET_COL = \"respuesta\" if TAREA == \"clasificacion\" else \"gasto_total\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga del dataset de inferencia\n",
    "\n",
    "Cargamos el dataset sobre el cual aplicaremos el modelo. Este debe tener la misma estructura de columnas que el dataset de entrenamiento, aunque pueden diferir en el número de observaciones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset cargado: 351 observaciones, 38 variables\n",
      "Variable objetivo disponible: True\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>nombre</th>\n",
       "      <th>apellidos</th>\n",
       "      <th>anio_nacimiento</th>\n",
       "      <th>direccion</th>\n",
       "      <th>telefono1</th>\n",
       "      <th>telefono2</th>\n",
       "      <th>email</th>\n",
       "      <th>dni</th>\n",
       "      <th>tarjeta_credito_asociada</th>\n",
       "      <th>...</th>\n",
       "      <th>acepta_cmp3</th>\n",
       "      <th>acepta_cmp4</th>\n",
       "      <th>acepta_cmp5</th>\n",
       "      <th>acepta_cmp1</th>\n",
       "      <th>acepta_cmp2</th>\n",
       "      <th>reclama</th>\n",
       "      <th>coste_contacto</th>\n",
       "      <th>ingresos_contacto</th>\n",
       "      <th>respuesta</th>\n",
       "      <th>usuario_alta_datos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>8e7ebfed1f1543fa81c81459dc89a4d9</td>\n",
       "      <td>651c1322dcc54f9196b6c9da25e6f979</td>\n",
       "      <td>1982.0</td>\n",
       "      <td>70b26c2715614ba19c8bf6f59c4680b4</td>\n",
       "      <td>c468424689a74a9d81cef105c57a5675</td>\n",
       "      <td>dc2f259a84c047b59269b4e71d4b0e26</td>\n",
       "      <td>7e810d39857942099e1a06f340ad8a09</td>\n",
       "      <td>0ca7475698ce48b39f5793790173e712</td>\n",
       "      <td>36751be5d859455fb7aa905eac7f0806</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>admin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24</td>\n",
       "      <td>9652fce167de476cbe7eb6006f4e10fe</td>\n",
       "      <td>c2ccf8175b73472dbffadd4d813fb411</td>\n",
       "      <td>1964.0</td>\n",
       "      <td>4d19f3d14360445e8e6093c56a0abd56</td>\n",
       "      <td>17ba13b5501a43bcae0bc4181c3eda90</td>\n",
       "      <td>3a4224bd3d1b4171b0f5df92ef73aff7</td>\n",
       "      <td>ebf1d2b586a44189a37413b078603efa</td>\n",
       "      <td>131da2e948634187a9baa76769f2621f</td>\n",
       "      <td>fe2ab2a89ee746f0985034939a4592c9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>juan.perez</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36</td>\n",
       "      <td>87c36b0440ce40088cf3a8730e72ca90</td>\n",
       "      <td>7153523e036d4dc4b167a2c64c3bad8d</td>\n",
       "      <td>NaN</td>\n",
       "      <td>b3636305bbdc4b1f827800f179487d9b</td>\n",
       "      <td>9c8683cfdc234741a739f7b23d8c743d</td>\n",
       "      <td>46b3057e7db84998acfeda811c3b6b86</td>\n",
       "      <td>97542b779f144d9fb054ae50ada28a24</td>\n",
       "      <td>a4c128bbc8d24a9680595d9b33ef0357</td>\n",
       "      <td>a0b00686b4174c79aceca58ad47a7e40</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>juan.perez</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40</td>\n",
       "      <td>432bc28e21cf4a4492efae9b8f2dd0c8</td>\n",
       "      <td>6a2b00e861794a46992593a151197fff</td>\n",
       "      <td>1963.0</td>\n",
       "      <td>4e2d9d499b5e46918c38e628f520027c</td>\n",
       "      <td>80d4f81e370e49a18362f4a7b48d9ad8</td>\n",
       "      <td>233bbe4abd8241a5bba8648694120f11</td>\n",
       "      <td>6085bc226bc0488da8661ac71dd1ca4e</td>\n",
       "      <td>2a2e3bf0f1494bf69234a153dfb77273</td>\n",
       "      <td>9f7d46da0c6f41d8b15e21677f6158d8</td>\n",
       "      <td>...</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>admin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>1bfa4d7aaca84a8d96785acb4412d9c1</td>\n",
       "      <td>09cfe74d95fc46a9908b114db7988141</td>\n",
       "      <td>1949.0</td>\n",
       "      <td>7f63c6885f4f45b58ca693e5a1dba298</td>\n",
       "      <td>abe303766efa422e9b7f59a6c7bd50d1</td>\n",
       "      <td>7a81508356334340bcdae67a03d28c57</td>\n",
       "      <td>2a95b42814964d438911905ce62ecc1c</td>\n",
       "      <td>59f4e9f9e01c43a184023868a6cd10e4</td>\n",
       "      <td>182117d9dba24c448e1ea24fa8f59f4b</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>juan.perez</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                            nombre                         apellidos  \\\n",
       "0   1  8e7ebfed1f1543fa81c81459dc89a4d9  651c1322dcc54f9196b6c9da25e6f979   \n",
       "1  24  9652fce167de476cbe7eb6006f4e10fe  c2ccf8175b73472dbffadd4d813fb411   \n",
       "2  36  87c36b0440ce40088cf3a8730e72ca90  7153523e036d4dc4b167a2c64c3bad8d   \n",
       "3  40  432bc28e21cf4a4492efae9b8f2dd0c8  6a2b00e861794a46992593a151197fff   \n",
       "4  41  1bfa4d7aaca84a8d96785acb4412d9c1  09cfe74d95fc46a9908b114db7988141   \n",
       "\n",
       "   anio_nacimiento                         direccion  \\\n",
       "0           1982.0  70b26c2715614ba19c8bf6f59c4680b4   \n",
       "1           1964.0  4d19f3d14360445e8e6093c56a0abd56   \n",
       "2              NaN  b3636305bbdc4b1f827800f179487d9b   \n",
       "3           1963.0  4e2d9d499b5e46918c38e628f520027c   \n",
       "4           1949.0  7f63c6885f4f45b58ca693e5a1dba298   \n",
       "\n",
       "                          telefono1                         telefono2  \\\n",
       "0  c468424689a74a9d81cef105c57a5675  dc2f259a84c047b59269b4e71d4b0e26   \n",
       "1  17ba13b5501a43bcae0bc4181c3eda90  3a4224bd3d1b4171b0f5df92ef73aff7   \n",
       "2  9c8683cfdc234741a739f7b23d8c743d  46b3057e7db84998acfeda811c3b6b86   \n",
       "3  80d4f81e370e49a18362f4a7b48d9ad8  233bbe4abd8241a5bba8648694120f11   \n",
       "4  abe303766efa422e9b7f59a6c7bd50d1  7a81508356334340bcdae67a03d28c57   \n",
       "\n",
       "                              email                               dni  \\\n",
       "0  7e810d39857942099e1a06f340ad8a09  0ca7475698ce48b39f5793790173e712   \n",
       "1  ebf1d2b586a44189a37413b078603efa  131da2e948634187a9baa76769f2621f   \n",
       "2  97542b779f144d9fb054ae50ada28a24  a4c128bbc8d24a9680595d9b33ef0357   \n",
       "3  6085bc226bc0488da8661ac71dd1ca4e  2a2e3bf0f1494bf69234a153dfb77273   \n",
       "4  2a95b42814964d438911905ce62ecc1c  59f4e9f9e01c43a184023868a6cd10e4   \n",
       "\n",
       "           tarjeta_credito_asociada  ... acepta_cmp3 acepta_cmp4  acepta_cmp5  \\\n",
       "0  36751be5d859455fb7aa905eac7f0806  ...         0.0         0.0          0.0   \n",
       "1  fe2ab2a89ee746f0985034939a4592c9  ...         0.0         0.0          0.0   \n",
       "2  a0b00686b4174c79aceca58ad47a7e40  ...         NaN         NaN          NaN   \n",
       "3  9f7d46da0c6f41d8b15e21677f6158d8  ...          no          no           no   \n",
       "4  182117d9dba24c448e1ea24fa8f59f4b  ...         0.0         0.0          0.0   \n",
       "\n",
       "   acepta_cmp1  acepta_cmp2 reclama  coste_contacto  ingresos_contacto  \\\n",
       "0          0.0          0.0     0.0             3.0               11.0   \n",
       "1          0.0          0.0     0.0             3.0               11.0   \n",
       "2          NaN          NaN     NaN             NaN                NaN   \n",
       "3           no           no     0.0             3.0               11.0   \n",
       "4          0.0          0.0     0.0             3.0               11.0   \n",
       "\n",
       "   respuesta  usuario_alta_datos  \n",
       "0        0.0               admin  \n",
       "1        0.0          juan.perez  \n",
       "2        NaN          juan.perez  \n",
       "3        0.0               admin  \n",
       "4        0.0          juan.perez  \n",
       "\n",
       "[5 rows x 38 columns]"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert DATA_FILE.exists(), f\"Dataset no encontrado: {DATA_FILE}\"\n",
    "\n",
    "df_raw = pd.read_csv(DATA_FILE)\n",
    "\n",
    "print(f\"Dataset cargado: {df_raw.shape[0]} observaciones, {df_raw.shape[1]} variables\")\n",
    "print(f\"Variable objetivo disponible: {TARGET_COL in df_raw.columns}\")\n",
    "df_raw.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de artefactos de entrenamiento\n",
    "\n",
    "Cargamos el pipeline entrenado y los metadatos que contienen información sobre las features esperadas, transformaciones aplicadas y configuración del modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline cargado: Pipeline\n",
      "Umbral de clasificación: 0.3500\n"
     ]
    }
   ],
   "source": [
    "assert METADATA_FILE.exists(), f\"Metadatos no encontrados: {METADATA_FILE}\"\n",
    "assert MODEL_FILE.exists(), f\"Modelo no encontrado: {MODEL_FILE}\"\n",
    "\n",
    "metadata = joblib.load(METADATA_FILE)\n",
    "model = joblib.load(MODEL_FILE)\n",
    "\n",
    "print(f\"Pipeline cargado: {type(model).__name__}\")\n",
    "if TAREA == \"clasificacion\":\n",
    "    print(f\"Umbral de clasificación: {metadata.get('clf_threshold', 0.5):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones auxiliares\n",
    "\n",
    "Definimos funciones para validar y preparar las features del dataset de inferencia, garantizando alineación exacta con el espacio de features utilizado durante el entrenamiento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    confusion_matrix,\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    "    r2_score,\n",
    ")\n",
    "\n",
    "def validar_columnas(work: pd.DataFrame, expected: list[str], contexto: str, allow_extras: bool = False) -> pd.DataFrame:\n",
    "    faltantes = [c for c in expected if c not in work.columns]\n",
    "    extras = [c for c in work.columns if c not in expected]\n",
    "\n",
    "    if faltantes:\n",
    "        msg = (\n",
    "            f\"Desalineación de columnas en {contexto}. \"\n",
    "            f\"Faltantes: {faltantes or 'ninguna'} | Extras: {extras or 'ninguna'}\"\n",
    "        )\n",
    "        raise ValueError(msg)\n",
    "\n",
    "    if extras and not allow_extras:\n",
    "        msg = (\n",
    "            f\"Desalineación de columnas en {contexto}. \"\n",
    "            f\"Faltantes: {faltantes or 'ninguna'} | Extras: {extras or 'ninguna'}\"\n",
    "        )\n",
    "        raise ValueError(msg)\n",
    "\n",
    "    if extras:\n",
    "        print(f\"Descartamos columnas extra en {contexto}: {extras}\")\n",
    "        work = work.drop(columns=extras)\n",
    "\n",
    "    return work[expected]\n",
    "\n",
    "\n",
    "def _first_existing(df: pd.DataFrame, candidates: list[str]) -> str | None:\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "\n",
    "def preprocesamiento_deterministico_01b(df: pd.DataFrame, target_col: str, meta: dict | None = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Replica lo determinístico de 01B y, si faltan columnas esperadas, intenta derivarlas\n",
    "    desde columnas base típicas (sin fits).\n",
    "    \"\"\"\n",
    "    work = df.copy()\n",
    "\n",
    "    # fechas / referencia  \n",
    "    if \"fecha_cliente\" in work.columns:\n",
    "        fechas = pd.to_datetime(work[\"fecha_cliente\"], errors=\"coerce\")\n",
    "\n",
    "        # Intento adicional para formatos DD-MM-YYYY como en los datos de inferencia\n",
    "        mask_na = fechas.isna() & work[\"fecha_cliente\"].notna()\n",
    "        if mask_na.any():\n",
    "            fechas_alt = pd.to_datetime(\n",
    "                work.loc[mask_na, \"fecha_cliente\"],\n",
    "                errors=\"coerce\",\n",
    "                dayfirst=True,\n",
    "            )\n",
    "            fechas.loc[mask_na] = fechas_alt\n",
    "\n",
    "        work[\"fecha_cliente\"] = fechas\n",
    "\n",
    "    # as_of_date (estable por archivo; evita usar \"hoy\" si hay fecha_cliente)\n",
    "    default_as_of = pd.Timestamp(\"2025-10-27\")\n",
    "    if meta and meta.get(\"as_of_date\"):\n",
    "        as_of = pd.to_datetime(meta[\"as_of_date\"])\n",
    "    else:\n",
    "        as_of = default_as_of\n",
    "    # edad (si falta)  \n",
    "    if \"edad\" not in work.columns:\n",
    "        col_birth = _first_existing(work, [\"anio_nacimiento\", \"Year_Birth\", \"year_birth\", \"birth_year\"])\n",
    "        if col_birth is not None:\n",
    "            birth = pd.to_numeric(work[col_birth], errors=\"coerce\")\n",
    "            work[\"edad\"] = (as_of.year - birth).astype(\"float\")\n",
    "\n",
    "    # anio_alta (si falta)  \n",
    "    if \"anio_alta\" not in work.columns and \"fecha_cliente\" in work.columns:\n",
    "        work[\"anio_alta\"] = work[\"fecha_cliente\"].dt.year\n",
    "\n",
    "    # antiguedad (si falta)  \n",
    "    if \"fecha_cliente\" in work.columns and work[\"fecha_cliente\"].notna().any():\n",
    "        if \"antiguedad_dias\" not in work.columns:\n",
    "            work[\"antiguedad_dias\"] = (as_of - work[\"fecha_cliente\"]).dt.days.astype(\"float\")\n",
    "        if \"antiguedad_anios\" not in work.columns:\n",
    "            work[\"antiguedad_anios\"] = (work[\"antiguedad_dias\"] / 365.25).astype(\"float\")\n",
    "\n",
    "    # estado_civil (01B): homogeneizar categorías EN->ES y validar  \n",
    "    if \"estado_civil\" in work.columns:\n",
    "        estados_validos = {\"Casado\", \"Soltero\", \"Divorciado\", \"Union_Libre\", \"Viudo\"}\n",
    "        estado_map = {\n",
    "            \"Married\": \"Casado\",\n",
    "            \"Together\": \"Union_Libre\",\n",
    "            \"Single\": \"Soltero\",\n",
    "            \"Divorced\": \"Divorciado\",\n",
    "            \"Widow\": \"Viudo\",\n",
    "            \"Alone\": \"Soltero\",\n",
    "        }\n",
    "        if work[\"estado_civil\"].dtype == object:\n",
    "            est_raw = work[\"estado_civil\"].astype(str).str.strip()\n",
    "            est_raw = est_raw.replace({\"nan\": np.nan, \"None\": np.nan, \"none\": np.nan, \"\": np.nan})\n",
    "            est_raw = est_raw.replace(estado_map)\n",
    "            desconocidos = sorted(set(est_raw.dropna().unique()) - estados_validos)\n",
    "            if desconocidos:\n",
    "                print(f\"Valores 'estado_civil' no mapeables (01B) -> se marcarán como NaN: {desconocidos}\")\n",
    "                est_raw = est_raw.where(~est_raw.isin(desconocidos))\n",
    "            work[\"estado_civil\"] = est_raw\n",
    "    # tiene_pareja (01B) con soporte de valores EN  \n",
    "    if \"estado_civil\" in work.columns and \"tiene_pareja\" not in work.columns:\n",
    "        pareja_vals = {\"Casado\", \"Union_Libre\", \"Married\", \"Together\"}\n",
    "        work[\"tiene_pareja\"] = work[\"estado_civil\"].isin(pareja_vals).astype(int)\n",
    "\n",
    "    # educacion (01B) con soporte de valores EN + 'nan' string  \n",
    "    if \"educacion\" in work.columns:\n",
    "        educ_map = {\n",
    "            \"Basica\": 1, \"Secundaria\": 2, \"Universitaria\": 3, \"Master\": 4, \"Doctorado\": 5,\n",
    "            # equivalencias EN (dataset marketing campaign)\n",
    "            \"Basic\": 1, \"2n Cycle\": 2, \"Graduation\": 3, \"PhD\": 5,\n",
    "        }\n",
    "        if work[\"educacion\"].dtype == object:\n",
    "            edu_raw = work[\"educacion\"].astype(str).str.strip()\n",
    "            edu_raw = edu_raw.replace({\"nan\": np.nan, \"None\": np.nan, \"none\": np.nan, \"NULL\": np.nan, \"null\": np.nan, \"\": np.nan})\n",
    "\n",
    "            unk = sorted(set(edu_raw.dropna().unique()) - set(educ_map.keys()))\n",
    "            if unk:\n",
    "                raise ValueError(f\"Valores 'educacion' no mapeables (01B): {unk}\")\n",
    "\n",
    "            work[\"educacion\"] = edu_raw.map(educ_map)\n",
    "\n",
    "    # hijos_casa (01B) con soporte EN  \n",
    "    if \"hijos_casa\" in work.columns and \"adolescentes_casa\" in work.columns:\n",
    "        work[\"hijos_casa\"] = pd.to_numeric(work[\"hijos_casa\"], errors=\"coerce\").fillna(0) + pd.to_numeric(work[\"adolescentes_casa\"], errors=\"coerce\").fillna(0)\n",
    "    elif \"Kidhome\" in work.columns and \"Teenhome\" in work.columns and \"hijos_casa\" not in work.columns:\n",
    "        work[\"hijos_casa\"] = pd.to_numeric(work[\"Kidhome\"], errors=\"coerce\").fillna(0) + pd.to_numeric(work[\"Teenhome\"], errors=\"coerce\").fillna(0)\n",
    "\n",
    "    # educacion_x_estado (01B)  \n",
    "    if {\"educacion\", \"tiene_pareja\"}.issubset(work.columns) and \"educacion_x_estado\" not in work.columns:\n",
    "        work[\"educacion_x_estado\"] = pd.to_numeric(work[\"educacion\"], errors=\"coerce\") * pd.to_numeric(work[\"tiene_pareja\"], errors=\"coerce\")\n",
    "\n",
    "    # gastos: derivar gasto_total / props / categorias_compradas / gasto_promedio si faltan  \n",
    "    gasto_candidates = {\n",
    "        \"gasto_vinos\": [\"gasto_vinos\", \"MntWines\"],\n",
    "        \"gasto_frutas\": [\"gasto_frutas\", \"MntFruits\"],\n",
    "        \"gasto_carnes\": [\"gasto_carnes\", \"MntMeatProducts\"],\n",
    "        \"gasto_pescado\": [\"gasto_pescado\", \"MntFishProducts\"],\n",
    "        \"gasto_dulces\": [\"gasto_dulces\", \"MntSweetProducts\"],\n",
    "        \"gasto_oro\": [\"gasto_oro\", \"MntGoldProds\"],\n",
    "    }\n",
    "\n",
    "    # normalizar nombres si vienen en EN y faltan en ES\n",
    "    for es, cands in gasto_candidates.items():\n",
    "        if es not in work.columns:\n",
    "            src = _first_existing(work, cands)\n",
    "            if src is not None:\n",
    "                work[es] = pd.to_numeric(work[src], errors=\"coerce\")\n",
    "\n",
    "    gasto_cols = [c for c in gasto_candidates.keys() if c in work.columns]\n",
    "\n",
    "    if \"gasto_total\" not in work.columns and gasto_cols:\n",
    "        work[\"gasto_total\"] = work[gasto_cols].fillna(0).sum(axis=1)\n",
    "\n",
    "    if \"categorias_compradas\" not in work.columns and gasto_cols:\n",
    "        work[\"categorias_compradas\"] = (work[gasto_cols].fillna(0) > 0).sum(axis=1).astype(int)\n",
    "\n",
    "    if \"gasto_promedio\" not in work.columns and \"gasto_total\" in work.columns:\n",
    "        # elección mínima y consistente: promedio por categoría comprada (evita duplicar ticket_promedio)\n",
    "        denom = work[\"categorias_compradas\"] if \"categorias_compradas\" in work.columns else 0\n",
    "        denom = pd.to_numeric(denom, errors=\"coerce\").fillna(0)\n",
    "        gt = pd.to_numeric(work[\"gasto_total\"], errors=\"coerce\").fillna(0)\n",
    "        work[\"gasto_promedio\"] = np.where(denom > 0, gt / denom, 0.0)\n",
    "\n",
    "    for es, _ in gasto_candidates.items():\n",
    "        prop = f\"prop_{es}\"\n",
    "        if prop not in work.columns and es in work.columns and \"gasto_total\" in work.columns:\n",
    "            num = pd.to_numeric(work[es], errors=\"coerce\").fillna(0)\n",
    "            den = pd.to_numeric(work[\"gasto_total\"], errors=\"coerce\").fillna(0)\n",
    "            work[prop] = np.where(den > 0, num / den, 0.0)\n",
    "\n",
    "    # compras: derivar compras_totales/offline/tasas/ticket si faltan  \n",
    "    col_web = _first_existing(work, [\"compras_web\", \"NumWebPurchases\", \"num_compras_web\"])\n",
    "    col_cat = _first_existing(work, [\"compras_catalogo\", \"NumCatalogPurchases\", \"num_compras_catalogo\"])\n",
    "    col_store = _first_existing(work, [\"compras_tienda\", \"NumStorePurchases\", \"num_compras_tienda\"])\n",
    "    col_deals = _first_existing(work, [\"compras_oferta\", \"NumDealsPurchases\", \"num_compras_oferta\"])\n",
    "\n",
    "    def _to_numeric(col_name: str | None) -> pd.Series | None:\n",
    "        if col_name and col_name in work.columns:\n",
    "            return pd.to_numeric(work[col_name], errors=\"coerce\").fillna(0)\n",
    "        return None\n",
    "\n",
    "    compras_web = _to_numeric(col_web)\n",
    "    compras_cat = _to_numeric(col_cat)\n",
    "    compras_store = _to_numeric(col_store)\n",
    "    compras_deals = _to_numeric(col_deals)\n",
    "\n",
    "    if \"compras_online\" in work.columns:\n",
    "        work[\"compras_online\"] = pd.to_numeric(work[\"compras_online\"], errors=\"coerce\").fillna(0)\n",
    "    elif compras_web is not None and compras_cat is not None:\n",
    "        work[\"compras_online\"] = (compras_web + compras_cat).astype(float)\n",
    "\n",
    "    if \"compras_totales\" not in work.columns:\n",
    "        componentes = [s for s in [compras_web, compras_cat, compras_store, compras_deals] if s is not None]\n",
    "        if componentes:\n",
    "            total = componentes[0]\n",
    "            for comp in componentes[1:]:\n",
    "                total = total + comp\n",
    "            work[\"compras_totales\"] = total.astype(float)\n",
    "\n",
    "    if \"compras_offline\" not in work.columns and compras_store is not None:\n",
    "        work[\"compras_offline\"] = compras_store.astype(float)\n",
    "\n",
    "    if \"ratio_compras_online\" not in work.columns and \"compras_online\" in work.columns and \"compras_totales\" in work.columns:\n",
    "        tot = pd.to_numeric(work[\"compras_totales\"], errors=\"coerce\").fillna(0)\n",
    "        online = pd.to_numeric(work[\"compras_online\"], errors=\"coerce\").fillna(0)\n",
    "        work[\"ratio_compras_online\"] = np.where(tot > 0, online / tot, np.nan)\n",
    "\n",
    "    if \"tasa_compra_online\" not in work.columns and \"ratio_compras_online\" in work.columns:\n",
    "        work[\"tasa_compra_online\"] = work[\"ratio_compras_online\"]\n",
    "\n",
    "    if \"tasa_compra_oferta\" not in work.columns and compras_deals is not None and \"compras_totales\" in work.columns:\n",
    "        deals = compras_deals\n",
    "        tot = pd.to_numeric(work[\"compras_totales\"], errors=\"coerce\").fillna(0)\n",
    "        work[\"tasa_compra_oferta\"] = np.where(tot > 0, deals / tot, 0.0)\n",
    "\n",
    "    if \"ticket_promedio\" not in work.columns and \"gasto_total\" in work.columns and \"compras_totales\" in work.columns:\n",
    "        gt = pd.to_numeric(work[\"gasto_total\"], errors=\"coerce\").fillna(0)\n",
    "        tot = pd.to_numeric(work[\"compras_totales\"], errors=\"coerce\").fillna(0)\n",
    "        work[\"ticket_promedio\"] = np.where(tot > 0, gt / tot, 0.0)\n",
    "    # hogar: tamano_hogar / dependientes / unipersonal si faltan  \n",
    "    if \"tamano_hogar\" not in work.columns:\n",
    "        if {\"tiene_pareja\", \"hijos_casa\"}.issubset(work.columns):\n",
    "            work[\"tamano_hogar\"] = (1 + pd.to_numeric(work[\"tiene_pareja\"], errors=\"coerce\").fillna(0) +\n",
    "                                    pd.to_numeric(work[\"hijos_casa\"], errors=\"coerce\").fillna(0)).astype(float)\n",
    "        elif \"total_dependientes\" in work.columns:\n",
    "            work[\"tamano_hogar\"] = (1 + pd.to_numeric(work[\"total_dependientes\"], errors=\"coerce\").fillna(0)).astype(float)\n",
    "\n",
    "    if \"tiene_dependientes\" not in work.columns:\n",
    "        if \"hijos_casa\" in work.columns:\n",
    "            work[\"tiene_dependientes\"] = (pd.to_numeric(work[\"hijos_casa\"], errors=\"coerce\").fillna(0) > 0).astype(int)\n",
    "        elif \"total_dependientes\" in work.columns:\n",
    "            work[\"tiene_dependientes\"] = (pd.to_numeric(work[\"total_dependientes\"], errors=\"coerce\").fillna(0) > 0).astype(int)\n",
    "\n",
    "    if \"hogar_unipersonal\" not in work.columns and \"tamano_hogar\" in work.columns:\n",
    "        work[\"hogar_unipersonal\"] = (pd.to_numeric(work[\"tamano_hogar\"], errors=\"coerce\").fillna(0) == 1).astype(int)\n",
    "\n",
    "    # reglas determinísticas 01B (filtrado)  \n",
    "    if \"edad\" in work.columns:\n",
    "        work = work[work[\"edad\"] <= 120].copy()\n",
    "    if \"ingresos\" in work.columns:\n",
    "        work = work[work[\"ingresos\"] != 666666].copy()\n",
    "\n",
    "    # ratio_compras_online: mismo criterio 01B (evita inf/NaN) -> dropna\n",
    "    if \"ratio_compras_online\" in work.columns:\n",
    "        work = work.dropna(subset=[\"ratio_compras_online\"]).copy()\n",
    "\n",
    "    # drop redundantes (01B)  \n",
    "    cols_drop = [\"total_dependientes\", \"compras_online\", \"adolescentes_casa\", \"anio_nacimiento\", \"fecha_cliente\"]\n",
    "    work = work.drop(columns=[c for c in cols_drop if c in work.columns])\n",
    "\n",
    "    # normalizar columnas binarias tipo campaña (acepta_cmp*, reclama, etc.) a 0/1 como en 01B  \n",
    "    if meta is not None:\n",
    "        bin_cols_clf = meta.get(\"binary_cols\", [])\n",
    "        bin_cols_reg = meta.get(\"reg_binary_cols\", [])\n",
    "        bin_cols_all = set(bin_cols_clf) | set(bin_cols_reg)\n",
    "\n",
    "        true_vals = {\"yes\", \"si\", \"sí\", \"true\", \"1\", \"y\", \"t\"}\n",
    "        false_vals = {\"no\", \"false\", \"0\", \"n\", \"f\"}\n",
    "\n",
    "        for col in bin_cols_all:\n",
    "            if col in work.columns and work[col].dtype == object:\n",
    "                s = work[col].astype(str).str.strip()\n",
    "                s_lower = s.str.lower()\n",
    "                mapped = np.where(\n",
    "                    s_lower.isin(true_vals), 1,\n",
    "                    np.where(s_lower.isin(false_vals), 0, np.nan)\n",
    "                )\n",
    "                # Intentar recuperar valores numéricos si vienen como strings \"0\"/\"1\"\n",
    "                num_fallback = pd.to_numeric(s, errors=\"coerce\")\n",
    "                mapped = np.where(np.isnan(mapped), num_fallback, mapped)\n",
    "                work[col] = pd.to_numeric(mapped, errors=\"coerce\")\n",
    "\n",
    "    # asegurar consistencia con entrenamiento: sin NaNs en features crudas usadas por el modelo  \n",
    "    # En 01B el modelo se entrenó sin NaNs en las columnas de entrada; aquí replicamos ese supuesto\n",
    "    if meta is not None:\n",
    "        expected_raw = meta.get(\"raw_feature_names\", [])\n",
    "        needed = [c for c in expected_raw if c in work.columns]\n",
    "        if needed:\n",
    "            before = len(work)\n",
    "            work = work.dropna(subset=needed).copy()\n",
    "            after = len(work)\n",
    "            if before != after:\n",
    "                print(f\"Filas descartadas por NaNs en features requeridas (01B): {before - after}\")\n",
    "\n",
    "    return work\n",
    "\n",
    "\n",
    "def preparar_features(df: pd.DataFrame, meta: dict) -> Tuple[pd.DataFrame, pd.Series | None]:\n",
    "    work = df.drop(columns=[TARGET_COL], errors=\"ignore\").copy()\n",
    "    expected = meta.get(\"raw_feature_names\", [])\n",
    "    work = validar_columnas(work, expected, \"clasificacion (features crudas)\", allow_extras=True)\n",
    "\n",
    "    for col in work.columns:\n",
    "        if str(work[col].dtype) == \"Int64\":\n",
    "            work[col] = work[col].astype(\"int64\")\n",
    "\n",
    "    target = df[TARGET_COL] if TARGET_COL in df.columns else None\n",
    "    return work, target\n",
    "\n",
    "\n",
    "def preparar_regresion(df: pd.DataFrame, meta: dict) -> Tuple[pd.DataFrame, pd.Series | None]:\n",
    "    work = df.drop(columns=[TARGET_COL], errors=\"ignore\").copy()\n",
    "    expected = meta.get(\"reg_raw_feature_names\") or meta.get(\"raw_feature_names\", [])\n",
    "    work = validar_columnas(work, expected, \"regresion (features crudas)\", allow_extras=True)\n",
    "\n",
    "    for col in work.columns:\n",
    "        if str(work[col].dtype) == \"Int64\":\n",
    "            work[col] = work[col].astype(\"int64\")\n",
    "\n",
    "    target = df[TARGET_COL] if TARGET_COL in df.columns else None\n",
    "    return work, target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparación del dataset para inferencia\n",
    "\n",
    "Extraemos las features y la variable objetivo (si está disponible), validando que las columnas coincidan con las esperadas por el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores 'estado_civil' no mapeables (01B) -> se marcarán como NaN: ['YOLO']\n",
      "Filas descartadas por NaNs en features requeridas (01B): 5\n",
      "Descartamos columnas extra en clasificacion (features crudas): ['id', 'nombre', 'apellidos', 'direccion', 'telefono1', 'telefono2', 'email', 'dni', 'tarjeta_credito_asociada', 'coste_contacto', 'ingresos_contacto']\n",
      "Features preparadas: 324 observaciones, 47 variables\n",
      "Target disponible: True\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>observaciones</th>\n",
       "      <th>features</th>\n",
       "      <th>target_disponible</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>324</td>\n",
       "      <td>47</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   observaciones  features  target_disponible\n",
       "0            324        47               True"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aplicar el mismo preprocesamiento determinístico de 01B antes de alinear columnas\n",
    "df_raw = preprocesamiento_deterministico_01b(df_raw, TARGET_COL, metadata)\n",
    "\n",
    "# Preparar features según tarea (alineación exacta a metadata)\n",
    "if TAREA == \"clasificacion\":\n",
    "    X_inf, y_inf = preparar_features(df_raw, metadata)\n",
    "elif TAREA == \"regresion\":\n",
    "    X_inf, y_inf = preparar_regresion(df_raw, metadata)\n",
    "else:\n",
    "    raise ValueError(f\"Tarea no reconocida: {TAREA}\")\n",
    "\n",
    "print(f\"Features preparadas: {X_inf.shape[0]} observaciones, {X_inf.shape[1]} variables\")\n",
    "print(f\"Target disponible: {y_inf is not None}\")\n",
    "\n",
    "resumen_dimensiones = pd.DataFrame({\n",
    "    \"observaciones\": [X_inf.shape[0]],\n",
    "    \"features\": [X_inf.shape[1]],\n",
    "    \"target_disponible\": [y_inf is not None]\n",
    "})\n",
    "resumen_dimensiones\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generación de predicciones\n",
    "\n",
    "Aplicamos el pipeline entrenado sobre el dataset de inferencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Umbral aplicado: 0.3500\n"
     ]
    }
   ],
   "source": [
    "if TAREA == \"clasificacion\":\n",
    "    y_pred_proba = model.predict_proba(X_inf)[:, 1]\n",
    "    clf_threshold = float(metadata.get(\"clf_threshold\", 0.5))\n",
    "    y_pred = (y_pred_proba >= clf_threshold).astype(int)\n",
    "\n",
    "    print(f\"Umbral aplicado: {clf_threshold:.4f}\")\n",
    "    distrib_pred = pd.Series(y_pred).value_counts().sort_index().to_frame(\"Frecuencia\")\n",
    "    distrib_pred[\"Porcentaje\"] = (distrib_pred[\"Frecuencia\"] / len(y_pred) * 100).round(2)\n",
    "    distrib_pred.index = distrib_pred.index.map({0: \"No Responde\", 1: \"Responde\"})\n",
    "    distrib_pred = distrib_pred.reindex([\"No Responde\", \"Responde\"], fill_value=0)\n",
    "    distrib_pred\n",
    "else:\n",
    "    # El pipeline de 01B en regresión fue entrenado sobre log1p(y). La predicción sale en log-escala.\n",
    "    y_pred_log = model.predict(X_inf)\n",
    "    y_pred = np.expm1(y_pred_log)\n",
    "\n",
    "    print(\"Estadísticas de las predicciones (EUROS):\")\n",
    "    resumen_pred = pd.DataFrame({\"y_pred\": pd.Series(y_pred).describe()})\n",
    "    resumen_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación de métricas\n",
    "\n",
    "Si el dataset incluye la variable objetivo, calculamos las métricas de rendimiento para evaluar la capacidad de generalización del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AUC</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>inferencia</th>\n",
       "      <td>0.942378</td>\n",
       "      <td>0.817901</td>\n",
       "      <td>0.465347</td>\n",
       "      <td>0.903846</td>\n",
       "      <td>0.614379</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 AUC  Accuracy  Precision    Recall        F1\n",
       "inferencia  0.942378  0.817901   0.465347  0.903846  0.614379"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pred 0</th>\n",
       "      <th>Pred 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Real 0</th>\n",
       "      <td>218</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Real 1</th>\n",
       "      <td>5</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Pred 0  Pred 1\n",
       "Real 0     218      54\n",
       "Real 1       5      47"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sugerencia: umbral que maximiza F1 en estos datos = 0.6000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>umbral_metadata</th>\n",
       "      <td>0.817901</td>\n",
       "      <td>0.465347</td>\n",
       "      <td>0.903846</td>\n",
       "      <td>0.614379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>umbral_sugerido</th>\n",
       "      <td>0.901235</td>\n",
       "      <td>0.661290</td>\n",
       "      <td>0.788462</td>\n",
       "      <td>0.719298</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Accuracy  Precision    Recall        F1\n",
       "umbral_metadata  0.817901   0.465347  0.903846  0.614379\n",
       "umbral_sugerido  0.901235   0.661290  0.788462  0.719298"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if y_inf is not None:\n",
    "    if TAREA == \"clasificacion\":\n",
    "        # (recalcular por seguridad)\n",
    "        y_pred_proba = model.predict_proba(X_inf)[:, 1]\n",
    "        clf_threshold = float(metadata.get(\"clf_threshold\", 0.5))\n",
    "        y_pred = (y_pred_proba >= clf_threshold).astype(int)\n",
    "\n",
    "        metrics_clf = pd.DataFrame({\n",
    "            \"AUC\": [roc_auc_score(y_inf, y_pred_proba)],\n",
    "            \"Accuracy\": [accuracy_score(y_inf, y_pred)],\n",
    "            \"Precision\": [precision_score(y_inf, y_pred, zero_division=0)],\n",
    "            \"Recall\": [recall_score(y_inf, y_pred, zero_division=0)],\n",
    "            \"F1\": [f1_score(y_inf, y_pred, zero_division=0)],\n",
    "        }, index=[\"inferencia\"])\n",
    "\n",
    "        matriz_confusion = pd.DataFrame(\n",
    "            confusion_matrix(y_inf, y_pred),\n",
    "            index=[\"Real 0\", \"Real 1\"],\n",
    "            columns=[\"Pred 0\", \"Pred 1\"],\n",
    "        )\n",
    "        display(metrics_clf)\n",
    "        display(matriz_confusion)\n",
    "\n",
    "        thresholds_grid = np.linspace(0.05, 0.95, 19)\n",
    "        thresholds = np.unique(np.concatenate(([clf_threshold], thresholds_grid)))\n",
    "        registros = []\n",
    "        for thr in thresholds:\n",
    "            y_tmp = (y_pred_proba >= thr).astype(int)\n",
    "            registros.append({\n",
    "                \"threshold\": thr,\n",
    "                \"Accuracy\": accuracy_score(y_inf, y_tmp),\n",
    "                \"Precision\": precision_score(y_inf, y_tmp, zero_division=0),\n",
    "                \"Recall\": recall_score(y_inf, y_tmp, zero_division=0),\n",
    "                \"F1\": f1_score(y_inf, y_tmp, zero_division=0),\n",
    "            })\n",
    "\n",
    "        df_thresholds = pd.DataFrame(registros).set_index(\"threshold\")\n",
    "        best_thr = df_thresholds[\"F1\"].idxmax()\n",
    "        print(f\"Sugerencia: umbral que maximiza F1 en estos datos = {best_thr:.4f}\")\n",
    "\n",
    "        comparacion_dict = {\"umbral_metadata\": df_thresholds.loc[clf_threshold]}\n",
    "        if abs(best_thr - clf_threshold) > 1e-9:\n",
    "            comparacion_dict[\"umbral_sugerido\"] = df_thresholds.loc[best_thr]\n",
    "        comparacion = pd.DataFrame(comparacion_dict).T\n",
    "        display(comparacion)\n",
    "\n",
    "    else:\n",
    "        # y_inf viene en euros (gasto_total). y_pred ya está en euros (expm1 aplicado).\n",
    "        baseline_mae = (y_inf - y_inf.mean()).abs().mean()\n",
    "        baseline_rmse = ((y_inf - y_inf.mean()) ** 2).mean() ** 0.5\n",
    "        mae = mean_absolute_error(y_inf, y_pred)\n",
    "        rmse = mean_squared_error(y_inf, y_pred) ** 0.5\n",
    "        r2 = r2_score(y_inf, y_pred)\n",
    "\n",
    "        metrics_reg = pd.DataFrame({\n",
    "            \"MAE\": [mae],\n",
    "            \"RMSE\": [rmse],\n",
    "            \"R2\": [r2],\n",
    "            \"baseline_MAE\": [baseline_mae],\n",
    "            \"baseline_RMSE\": [baseline_rmse],\n",
    "        }, index=[\"inferencia\"])\n",
    "\n",
    "        display(metrics_reg)\n",
    "        distrib_pred = pd.DataFrame({\n",
    "            \"y_real\": y_inf.describe(),\n",
    "            \"y_pred\": pd.Series(y_pred).describe(),\n",
    "        })\n",
    "        distrib_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferencia completada.\n",
      "Tarea: clasificacion\n",
      "Filas inferidas: 324\n"
     ]
    }
   ],
   "source": [
    "print(\"Inferencia completada.\")\n",
    "print(f\"Tarea: {TAREA}\")\n",
    "print(f\"Filas inferidas: {len(X_inf)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Síntesis\n",
    "\n",
    "Cargamos los pipelines entrenados y aplicamos las predicciones sobre el dataset de inferencia. Si está disponible la variable objetivo, calculamos las métricas correspondientes para evaluar el rendimiento del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones\n",
    "\n",
    "El notebook carga el pipeline entrenado y genera predicciones sobre el dataset de inferencia. Las métricas obtenidas permiten evaluar el desempeño del modelo sobre datos nuevos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
